---
title: "Divide and Conquer Algorithm —— Version 1"
output:
pdf_document: default
html_document: default
date: "2022-11-30"
---

# 0. Before the algorithm

Before we start our divide and conquer algorithm, we need to import some packaged that used in this R exercise.

``` r
install.packages('dirichletprocess')  # This package is used for performing DP clustering
install.packages('dbscan')            # This package is used for performing DBSCAN algorithm
```

The `DBSCAN` is Density-based spatial clustering of applications with noise, and the general steps for this clustering algorithm can be seen below:

    DBSCAN(DB, distFunc, eps, minPts) {
        C := 0                                              # Cluster counter
        for each point P in database DB {
            if label(P) ≠ undefined then continue           # Previously processed in inner loop
            Neighbors N := RangeQuery(DB, distFunc, P, eps) # Find neighbors
            if |N| < minPts then {                          # Density check
                label(P) := Noise                           # Label as Noise
                continue
            }
            C := C + 1                                      # next cluster label
            label(P) := C                                   # Label initial point
            SeedSet S := N \ {P}                            # Neighbors to expand
            for each point Q in S {                         # Process every seed point Q
                if label(Q) = Noise then label(Q) := C      # Change Noise to border point
                if label(Q) ≠ undefined then continue   # Previously processed (e.g., border point)
                label(Q) := C                               # Label neighbor
                Neighbors N := RangeQuery(DB, distFunc, Q, eps) # Find neighbors
                if |N| ≥ minPts then {                      # Density check (if Q is a core point)
                    S := S ∪ N                              # Add new neighbors to seed set
                }
            }
        }
    }

```{r, include=FALSE}
# Import all the required packages
library(MASS) # Simulate from a Multivariate Normal Distribution
library(ggplot2)
library(dirichletprocess) # Dirichlet Process
library(dbscan) # Density-based Clustering -- DBSCAN
library(dplyr)
library(class)
```

And also change the path of program which can make it easier for saving and loading data.

```{r, include=FALSE}
# Set the path that we needed
path <- paste(getwd(), '/DC_data/', sep = '')
path <- paste('/Users/awc789/Library/CloudStorage/OneDrive-UniversityofStAndrews/GitHub/Notebook/R_code', '/DC_data/', sep = '')

if(!file.exists(path)){
  dir.create(path)
}
setwd(path)
```

Finally, determine the initial parameters of both worker numbers and the size of the entire dataset. In this exercise, we have $20$ workers and, for each worker, it has $50,000$ data in each distribution. So we would have $1,000,000$ data of each distribution with $8$ distribution of total $8,000,000$ data.

```{r}
# The Synthetic Data Setup
workers_num <- 20   # Have 20 workers
partition <- 50000  # Each distribution generate 50,000 samples in each worker
num <- partition * workers_num  # The total sample number from each distribution (1 million)
```

# 1. Generate the initial data with 2-D Normal Distribution

We using $8$ different $2-D$ Normal distributions to generate samples with mean and variance below:

$$
（\mu_1, \mu_2, \cdot \cdot \cdot, \mu_8)  =
\begin{pmatrix}
6 & 8 & 20 & 22 & 1.5 & 6 & 8 & 21 \\
4 & 22.5 & 22 & 6.5 & 6 & 1.5 & 31 & 29\\
\end{pmatrix}
$$

and

$$
\sum_1 = \begin{pmatrix}4.84 & 0 \\ 0 & 2.89 \end{pmatrix}, \sum_2 = \begin{pmatrix}4.84 & 0 \\ 0 & 2.89 \end{pmatrix}, \sum_3 = \begin{pmatrix} 3.61 & -5.05 \\ -5.05 & 14.44 \end{pmatrix},  \sum_4= \begin{pmatrix}12.25 & 0 \\ 0 & 3.24 \end{pmatrix}
$$

$$
\sum_5 = \begin{pmatrix}3.24 & 0 \\ 0 & 12.25 \end{pmatrix}, \sum_6 = \begin{pmatrix}14.44 & 0 \\ 0 & 2.25 \end{pmatrix}, \sum_7 = \begin{pmatrix}2.25 & 0 \\ 0 & 17.64 \end{pmatrix}, \sum_8 = \begin{pmatrix}2.25 & 4.2 \\ 4.2 & 16 \end{pmatrix}
$$

Next we separate 8 Normal distributions into 4 classes and combine them together with weight $W_{GMM} = ({\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4}})$ and $\omega_1 = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})$, $\omega_2 = (\frac{1}{2}, \frac{1}{2})$, $\omega_3 = (\frac{1}{2}, \frac{1}{2})$, $\omega_4 = (1)$

The weight $W_{GMM}$ is the weight for $4$ classes,

and the weight $\omega_i, i \in{1,2,3,4}$ is the weight for $8$ Normal distributions to combine them into $4$ classes;

This idea of the mixture of mixtures is from the paper "Distributed Bayesian clustering using finite mixture of mixtures" which we had reading in for our first reading meeting.

```{r}
miu_matirx <- matrix(c(6, 4, 8, 22.5, 20, 22, 22, 6.5, 1.5, 6, 6, 1.5, 8, 31, 21, 29),
                     nrow = 2, ncol = 8, byrow=FALSE)

covar_matrix <-array(c(4.84, 0, 0, 2.89, 3.61, 5.05, 5.05, 14.44, 3.61, -5.05,
                       -5.05, 14.44, 12.25, 0, 0, 3.24, 3.24, 0, 0, 12.25,
                       14.44, 0, 0, 2.25, 2.25, 0, 0, 17.64, 2.25, 4.2, 4.2, 16),
                     dim = c(2, 2, 8))

weight_GMM <- c(1/4, 1/4, 1/4, 1/4)
w1 <- c(1/3, 1/3 ,1/3)
w2 <- c(1/2, 1/2)
w3 <- c(1/2, 1/2)
w4 <- c(1)
```

## 1.1 Eight Normal Distributions

Generate the data, and also label the data.

```{r}
## Generate the Initial Data
x_1 <- data.frame(mvrnorm(n = num, miu_matirx[,1], covar_matrix[, , 1]))
x_6 <- data.frame(mvrnorm(n = num, miu_matirx[,2], covar_matrix[, , 2]))
x_4 <- data.frame(mvrnorm(n = num, miu_matirx[,3], covar_matrix[, , 3]))
x_8 <- data.frame(mvrnorm(n = num, miu_matirx[,4], covar_matrix[, , 4]))
x_2 <- data.frame(mvrnorm(n = num, miu_matirx[,5], covar_matrix[, , 5]))
x_3 <- data.frame(mvrnorm(n = num, miu_matirx[,6], covar_matrix[, , 6]))
x_7 <- data.frame(mvrnorm(n = num, miu_matirx[,7], covar_matrix[, , 7]))
x_5 <- data.frame(mvrnorm(n = num, miu_matirx[,8], covar_matrix[, , 8]))

## Give Data lables
x_1$component = rep(1,num)
x_2$component = rep(1,num)
x_3$component = rep(1,num)
x_4$component = rep(4,num)
x_5$component = rep(4,num)
x_6$component = rep(8,num)
x_7$component = rep(8,num)
x_8$component = rep(12,num)

## Give Data sub_lables
x_1$subcomponent = rep(1,num)
x_2$subcomponent = rep(2,num)
x_3$subcomponent = rep(3,num)
x_4$subcomponent = rep(4,num)
x_5$subcomponent = rep(5,num)
x_6$subcomponent = rep(6,num)
x_7$subcomponent = rep(7,num)
x_8$subcomponent = rep(8,num)
```

Not used in this algorithm, but still generate the mixture model result:

```{r}
GMM_1 <- w1[1] * x_1 + w1[2] * x_2 + w1[3] * x_3
GMM_2 <- w2[1] * x_4 + w2[2] * x_5
GMM_3 <- w3[1] * x_6 + w3[2] * x_7
GMM_4 <- w4[1] * x_8

Mixture_GMM <- weight_GMM[1] * GMM_1 + weight_GMM[2] * GMM_2 + weight_GMM[3] * GMM_3 + weight_GMM[4] * GMM_4
```

## 1.2 Plot the 2-D figure

Next plot the data in a $2-D$ figure with using $500$ data in each Normal distribution

```{r}
num_sample <- 500 # the number of date used for ploting
x_mix <- rbind(x_1[1:num_sample,], x_2[1:num_sample,])
x_mix <- rbind(x_mix, x_3[1:num_sample,])
x_mix <- rbind(x_mix, x_4[1:num_sample,])
x_mix <- rbind(x_mix, x_5[1:num_sample,])
x_mix <- rbind(x_mix, x_6[1:num_sample,])
x_mix <- rbind(x_mix, x_7[1:num_sample,])
x_mix <- rbind(x_mix, x_8[1:num_sample,])

ggplot(x_mix, aes(x = X1, y = X2, colour = component)) +
  geom_point(size=3)
```

```{r}
GMM_mix <- rbind(GMM_1[1:num_sample,], GMM_2[1:num_sample,])
GMM_mix <- rbind(GMM_mix, GMM_3[1:num_sample,])
GMM_mix <- rbind(GMM_mix, GMM_4[1:num_sample,])

ggplot(GMM_mix, aes(x = X1, y = X2, colour = component)) +
  geom_point(size=3)
```

```{r}
Mixture_GMM$component = rep(-10,num)
x_mix_with_GMM <- rbind(x_mix, Mixture_GMM[1:num_sample,])

ggplot(x_mix_with_GMM, aes(x = X1, y = X2, colour = component)) +
  geom_point(size=3)
```

## 1.3 Split the data and save to each worker

Save the whole data in Master Level

```{r}
write.table(x_1, file = paste(path, "x_1_total.txt", sep=''), sep = ",")
write.table(x_2, file = paste(path, "x_2_total.txt", sep=''), sep = ",")
write.table(x_3, file = paste(path, "x_3_total.txt", sep=''), sep = ",")
write.table(x_4, file = paste(path, "x_4_total.txt", sep=''), sep = ",")
write.table(x_5, file = paste(path, "x_5_total.txt", sep=''), sep = ",")
write.table(x_6, file = paste(path, "x_6_total.txt", sep=''), sep = ",")
write.table(x_7, file = paste(path, "x_7_total.txt", sep=''), sep = ",")
write.table(x_8, file = paste(path, "x_8_total.txt", sep=''), sep = ",")

write.table(Mixture_GMM, file = paste(path, "Mixture_GMM_total.txt", sep=''), sep = ",")
```

Save the splited data in Workers Level

```{r}
name_x <- paste("x_", 1:8, sep = "")
name_workers <- paste("worker_", 1:20, sep = "")
count <- 0

for(i in list(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8)){
  count <- count +1
  current_x <- name_x[count]
  # For the workers
  for(k in rep(1:workers_num)){
    current_worker <- name_workers[k]
    folder <- paste(path, '/', current_worker, sep = "")
    # check the folder
    if(!file.exists(folder)){
      dir.create(folder)
    }
    # for each workers
    start_loop <- partition * (k-1) + 1
    end_loop <- partition * k
    temp <- i[start_loop:end_loop,]
    f <- paste(path, current_worker, '/', current_x, '_', current_worker, '.txt', sep = "")
    write.table(temp, file = f, sep = ",")
  }
}


for(k in rep(1:workers_num)){
  current_worker <- name_workers[k]
  folder <- paste(path, '/', current_worker, sep = "")
  # for each workers
  start_loop <- partition * (k-1) + 1
  end_loop <- partition * k
  temp <- i[start_loop:end_loop,]
  f <- paste(path, current_worker, '/', 'Mixture_GMM_', current_worker, '.txt', sep = "")
  write.table(temp, file = f, sep = ",")
}
```

# 2. Divide-and-Conquer in Workers

```{r}
name_x <- paste("x_", 1:8, sep = "")
name_workers <- paste("worker_", 1:20, sep = "")
```

## 2.1 DP for the clustering

In this part, we perform Dirichlet Process for data clustering using the package `library(dirichletprocess)` at each Worker. This process should be done in parallel, but as I only have one computer, so I have switched to using a loop to obtain the same results. And this can take a considerable amount of time.

```{r}
# By change the value of these 2 parameters smaller, it can reduce the computational time

num_dp <- 2400    # The number of subsamping for DP clustering
MCMC_mum <- 1500  # The number of samples obtained during MCMC
```

And, consider we now have $20$ workers, but in each worker we have $50,000 \times 8 = 400,000$ data which is still a quite a huge amount of samples. So before doing a DP clustering, we do a subsampling of $2400$ data and use MCMC to sampling another $1500$ data for the Dirichlet Process. All of these steps are all encapsulated in functions of `DirichletProcessMvnormal()` and `Fit()`, and the function `Fit()` has MCMC inside.

## 2.2 Clustering classes reduction

As the DP clustering result might have many clustering classes than we needed which depended on the input number of data. And some of the clustering would only have $5$ or $6$ data points and we can directly drop these labels of clustering. By using this method, we can reduce the number of classes of DP clustering, and would only remaining with $4-5$ clustering classes.

## 2.3 Calculate the central points

Then we calculate the the central point of each clustering class, and save them for each worker. And due to we have $20$ workers, so it would eventually obtained with $80-100$ central points. If you think the central points are not enough, we can repeat the whole process above for another round and obtain more central points.

```{r}
## The parameters below are just for a quick presentation.
name_workers <- paste("worker_", 1:20, sep = "")

#num_dp <- 2000
#MCMC_mum <- 1000

num_dp <- 200
MCMC_mum <- 20

for(current_worker in name_workers){
  data <- data.frame()
  for (current_x in name_x) {
    f <- paste(path, current_worker, '/', current_x, '_', current_worker, '.txt', sep = "")
    data_temp <- read.table(f, sep = ',', header = TRUE)
    data <- rbind(data, data_temp)
  }

  # --------------------------------------------
  # DP for the clustering
  data <- data[sample(1:nrow(data)),]
  data_dp <- data[1:num_dp,]
  data_dp <- matrix(c(data_dp$X1, data_dp$X2), ncol = 2, byrow = FALSE)

  dpCluster <-  DirichletProcessMvnormal(data_dp)
  dpCluster <- Fit(dpCluster, MCMC_mum, progressBar = FALSE)
  plot(dpCluster)

  data_dp_cluster <- data[1:num_dp,]
  data_dp_cluster$clusterLabels <- dpCluster$clusterLabels

  label_result <- t(data.frame(table(dpCluster$clusterLabels)))
  drop_clusterlabels <- c()
  for (i in seq(ncol(label_result))) {
    if(label_result[2,i] <= (nrow(data_dp_cluster)/ncol(label_result))){
      drop_clusterlabels <- append(drop_clusterlabels, i)
      data_dp_cluster <- data_dp_cluster[data_dp_cluster$clusterLabels != i,]
    }
  }

  remain_clusterlabels <- data.frame(table(data_dp_cluster$clusterLabels))
  center_point <- data.frame(X1 = c(1:nrow(remain_clusterlabels)),
                             X2 = c(1:nrow(remain_clusterlabels)),
                             clusterlabels = c(1:nrow(remain_clusterlabels)))

  remain_clusterlabels <- remain_clusterlabels$Var1
  for(i in remain_clusterlabels){
    i <- as.numeric(i)
    temp_X1 <- data_dp_cluster[data_dp_cluster$clusterLabels == i,]$X1
    temp_X2 <- data_dp_cluster[data_dp_cluster$clusterLabels == i,]$X2

    center_point[i,1] <- mean(temp_X1)
    center_point[i,2] <- mean(temp_X2)
    center_point[i,3] <- i
  }


  folder <- paste(path, '/Center_point/', sep = "")
  # check the folder
  if(!file.exists(folder)){
    dir.create(folder)
  }

  f <- paste(path, 'Center_point/Center_point_', current_worker, '.txt', sep = "")
  write.table(center_point, file = f, sep = ",")

}
```

The below is the clustering result of the DP

```{r}
plot(dpCluster)
```

# 3. Divide-and-Conquer in Master

```{r}
name_x <- paste("x_", 1:8, sep = "")
name_workers <- paste("worker_", 1:20, sep = "")
```

## 3.1 Central points label refinement

In the $Section 2.3$, we calculate the central points of $20$ workers. However, due to the DP clustering with classes reduction in $Section 2.2$, all the labels of central points might be vary even if there are refer to the same data, so it would be hard to base these labels to perform a alignment in Master Level.

```{r}
data <- data.frame()
for(current_worker in name_workers){
  f <- paste(path, 'Center_point/Center_point_', current_worker, '.txt', sep = "")
  data_temp <- read.table(f, sep = ',', header = TRUE)
  data <- rbind(data, data_temp)
}

ggplot(data, aes(x = X1, y = X2, colour = clusterlabels)) +
  geom_point(size=3)
```

The above figure shows the location of each central point, but they are with different labels. But we can find out they they are generally mainly in $4$ areas, so we can perform a `DBSCAN` clustering below:

```{r}
data_central <- matrix(c(data$X1, data$X2), ncol = 2, byrow = FALSE)
db = dbscan(data_central, 5, 4)
hullplot(data_central, db$cluster)
```

```{r}
data$clusterlabels <- db$cluster
ggplot(data, aes(x = X1, y = X2, colour = clusterlabels)) +
  geom_point(size=3)

# ---------------------------------------------------------------------------------------------
center_point <- data.frame(X1 = c(1:nrow(data.frame(table(data$clusterlabels)))),
                           X2 = c(1:nrow(data.frame(table(data$clusterlabels)))),
                           clusterlabels = c(1:nrow(data.frame(table(data$clusterlabels)))))
for(i in data.frame(table(data$clusterlabels))$Var1){
  i <- as.numeric(i)
  temp_X1 <- data[data$clusterlabels == i,]$X1
  temp_X2 <- data[data$clusterlabels == i,]$X2

  center_point[i,1] <- mean(temp_X1)
  center_point[i,2] <- mean(temp_X2)
  center_point[i,3] <- i
}
```

The DBSACN clustering algorithm generally classify the whole dataset of central points into $4$ categories. And for each category, we give it a new label and regard them refer to the central points of the same dataset. Then we calculate the central point of these central points with the same label, and this will be an important part of the clustering strategy of Master Level clustering.

## 3.2 Master Level's Clustering

For the Master Level clustering, we have an idea which is similar to KNN and K-Means, but not exactly the same. As we have obtained $4$ final central points, so now we calculate the distance of each data point to $4$ final central points, and pick the smallest distance and denote this data point is in the same clustering of this central point.

```{r}
# This is the function of master clustering
master_clustering <- function(data, center){
  data_result <- data.frame()
  for(i in seq(nrow(data))){
    X1 <- data[i,]$X1
    X2 <- data[i,]$X2
    data_result[i,1] <- X1
    data_result[i,2] <- X2
    for(j in seq(nrow(center))){
      distance <- sqrt((X1 - center[j,]$X1)^2 + (X2 - center[j,]$X2)^2)
      data_result[i,j+2] <- distance
    }
  }
  k <- ncol(data_result)
  distace_name <- c(1:(k-2))
  colnames(data_result) <- c('X1', 'X2', distace_name)
  data_result$clusterlabels <- colnames(data_result[,3:k])[apply(data_result[,3:k],1,which.min)]
  return(data_result)
}
```

Using $8,000$ data points as an example below:

```{r}
data_master <- data.frame()
for(current_x in name_x){
  f <- paste(path, current_x, '_total.txt', sep = "")
  data_temp <- read.table(f, sep = ',', header = TRUE)
  data_master <- rbind(data_master, data_temp)
}

sample_size <- 8000
data_master <- data_master[sample(1:nrow(data_master)), ]
data_master_sample <- data_master[1:sample_size, ]
master_result <- master_clustering(data_master_sample, center_point)

ggplot(master_result, aes(x = X1, y = X2, colour = clusterlabels)) +
  geom_point(size=3)

```

# 4. Discuss

Although, now we can give a general algorithm of Divide-and-Conquer clustering, it still have many problems and the biggest one would be that this algorithm currently have no `Loss Function` . And the clustering result are not very well, it would be better if we can add curve lines during the clustering.
